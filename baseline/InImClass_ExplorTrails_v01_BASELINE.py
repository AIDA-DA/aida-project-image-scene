# -*- coding: utf-8 -*-
"""Kopie von InImClass_ExplorTrails_v01-baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ug1rhXCYkqHxs1jbke_PArs01OwBgdz8

# 1.0 Imported libs
"""

# Imports
import tensorflow 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, MaxPool2D, Dense, Flatten, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.applications import MobileNetV2
from keras.preprocessing import image
import os
import pathlib
import numpy as np
import matplotlib.pyplot as plt

"""# 1.1 Download & Prepare Dataset"""

'''
os.environ["KAGGLE_CONFIG_DIR"] = "/content"
os.chmod("/content", 600)
os.chmod("/content/kaggle.json", 600)

#Authenticate and Download Dataset from Kaggle
from kaggle.api.kaggle_api_extended import KaggleApi
api = KaggleApi()
api.authenticate()

api.dataset_download_files('puneet6060/intel-image-classification', unzip=True)
'''

"""# 1.2. Data import infos
Have a first look how many images and classes are in the dataset:
https://www.kaggle.com/puneet6060/intel-image-classification
"""

#train_dir = pathlib.Path('/content/seg_train/seg_train/')     # colab
#test_dir = pathlib.Path('/content/seg_test/seg_test/')        # colab

import pathlib
train_dir = pathlib.Path('./content/intel_image/seg_train/')   # local
test_dir = pathlib.Path('./content/intel_image/seg_test/')     # local

print(f"We have < {len(list(train_dir.glob('*/*.jpg')))} > images in < TRAIN SET > ({train_dir})")
print(f"We have < {len(list(test_dir.glob('*/*.jpg')))} > images in < TEST SET > ({test_dir})")

class_names = list([item.name for item in train_dir.glob('*')])
#class_names = list([item.name for item in test_dir.glob('*')])
print("We have the following classes:", class_names)

"""# 1.3 Image Data Generator (Keras)"""

batch_size = 32

image_generator = ImageDataGenerator(rescale=1./255)

train_generator = image_generator.flow_from_directory(train_dir,
                                                      target_size = (150,150),
                                                      batch_size=batch_size,
                                                      class_mode='categorical')

test_generator = image_generator.flow_from_directory(test_dir,
                                                     target_size=(150,150),
                                                     batch_size=batch_size,
                                                     class_mode='categorical')

"""# 1.4 First Data Exploration"""

def show_batch(image_batch, label_batch):
    plt.figure(figsize=(10,10))
    for n in range(10):
        ax = plt.subplot(5,5,n+1)
        plt.imshow(image_batch[n])
        plt.title(np.array(class_names)[label_batch[n]==1][0].title())
        plt.axis('off')

image_batch, label_batch = next(train_generator)
show_batch(image_batch, label_batch)

"""# 2.0 First trail with simple Neural Network"""

# Parameter
parameter = 'first simple Neural Network'
steps_per_epoch=len(train_generator.filepaths)//batch_size
validation_steps= len(test_generator.filepaths)//batch_size
epochs = 1      # 50
verbose = 1     # 1
print('Parameter -', parameter)
print('steps_per_epoch ', steps_per_epoch)
print('validation_steps ', validation_steps)
print('epochs ', epochs)
print('verbose ', verbose)

early_stopping = EarlyStopping(monitor='val_loss', patience=3)

# Create model adding different layers.
# Using 'softmax' activation because we have a multiclass classification.
model = Sequential([Flatten(),
                    Dense(512, activation = 'relu'),
                    Dense(256, activation = 'relu'),
                    Dropout(rate=0.5),
                    Dense(6, activation = 'softmax')])

# Compile the model
# loss: categorical_crossentropy because the targets are one-hot encoded.
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Fitting the model training with train test and validate test set.
# Callback: early_stopping

trained_MLP = model.fit(train_generator, 
                    validation_data = test_generator,
                    epochs = epochs,
                    verbose = verbose,
                    callbacks= [early_stopping],
                    steps_per_epoch=steps_per_epoch,
                    validation_steps=validation_steps);

# Save weights
model.save_weights('weights_MLP.h5')

def plot_acc_loss(trained):
    fig, ax = plt.subplots(1, 2, figsize=(15,5))
    ax[0].set_title('loss')
    ax[0].plot(trained.epoch, trained.history["loss"], label="Train loss")
    ax[0].plot(trained.epoch, trained.history["val_loss"], label="Validation loss")
    ax[1].set_title('acc')
    ax[1].plot(trained.epoch, trained.history["accuracy"], label="Train acc")
    ax[1].plot(trained.epoch, trained.history["val_accuracy"], label="Validation acc")
    ax[0].legend()
    ax[1].legend()

plot_acc_loss(trained_MLP)

#Loading weights
model.load_weights('weights_MLP.h5')

# Evaluate the model with the test set.
model_MLP_score = model.evaluate(test_generator, steps=validation_steps)
print("Model MLP Test Loss:", model_MLP_score[0])
print("Model MLP Test Accuracy:", model_MLP_score[1])

"""# 3.0 CNN Model"""

# Parameter
parameter = 'CNN Model'
steps_per_epoch=len(train_generator.filepaths)//batch_size
validation_steps= len(test_generator.filepaths)//batch_size
epochs = 1      # 40
verbose = 1     # 1
print('Parameter -', parameter)
print('steps_per_epoch', steps_per_epoch)
print('validation_steps', validation_steps)
print('epochs', epochs)
print('verbose ', verbose)


# Create the model adding Conv2D. 
model = Sequential([Conv2D(200, (3,3), activation='relu', input_shape=(150, 150, 3)),
                    MaxPool2D(5,5),
                    Conv2D(180, (3,3), activation='relu'),
                    MaxPool2D(5,5),
                    Flatten(),
                    Dense(180, activation='relu'),
                    Dropout(rate=0.5),
                    Dense(6, activation='softmax')])

# Compile model.
# Loss categorical_crossentropy: targets are one-hot encoded.
model.compile(optimizer='adam', 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Fitting the model training with train test and validate test set.
# Callback: early_stopping
trained_CNN = model.fit(train_generator,
                    validation_data = test_generator,
                    epochs = epochs,
                    verbose = 1,
                    callbacks= [early_stopping],
                    steps_per_epoch=steps_per_epoch,
                    validation_steps=validation_steps);

# Save weights
model.save_weights('weights_CNN.h5')

plot_acc_loss(trained_CNN)

# Load weights and evaluate model
model.load_weights('weights_CNN.h5')
model_CNN_score = model.evaluate(test_generator)
print("Model CNN Test Loss:", model_CNN_score[0])
print("Model CNN Test Accuracy:", model_CNN_score[1])

"""# 6.0 Apply Data Augmentation
A brief explication of used parameter:

* **Shear_range:** for randomly applying shearing transformations.
* **Zoom_range:** for randomly zooming inside pictures.
* **Horizontal_flip:** for randomly flipping half of the images horizontally.


"""

# Parameter
parameter = 'Data Augmentation'
steps_per_epoch=len(train_generator.filepaths)//batch_size
validation_steps= len(test_generator.filepaths)//batch_size
epochs = 1      # 40
verbose = 1     # 1
print('Parameter -', parameter)
print('steps_per_epoch', steps_per_epoch)
print('validation_steps', validation_steps)
print('epochs', epochs)
print('verbose ', verbose)

# Create ImageDataGenerator with new parameters for Data Augmentation
image_generator = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,  
    zoom_range=0.2,        
    horizontal_flip=True,
    validation_split=0.3)

train_generator = image_generator.flow_from_directory(train_dir,
                                                      target_size = (150,150),
                                                      batch_size=32,
                                                      shuffle=True,
                                                      seed=10,
                                                      class_mode='categorical')

test_generator = image_generator.flow_from_directory(test_dir,
                                                     target_size=(150,150),
                                                     batch_size=32,
                                                     shuffle=True,
                                                     seed=10,
                                                     class_mode='categorical')

# Create the same model as the previous one
model = Sequential([Conv2D(200, (3,3), activation='relu', input_shape=(150, 150, 3)),
                    MaxPool2D(5,5),
                    Conv2D(180, (3,3), activation='relu'),
                    MaxPool2D(5,5),
                    Flatten(),
                    Dense(180, activation='relu'),
                    Dropout(rate=0.5),
                    Dense(6, activation='softmax')])

model.compile(optimizer='adam', 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Fitting the model training with train test and validate test set.
# Callback: early_stopping
trained_DA = model.fit(train_generator,
                    validation_data  = test_generator,
                    epochs = epochs,
                    verbose = verbose,
                    callbacks= [early_stopping])

# Save weights
model.save_weights('weights_CNN_DA.h5')

plot_acc_loss(trained_DA)

# Load weights and evaluate model
model.load_weights('weights_CNN_DA.h5')
model_DA_score = model.evaluate(test_generator)
print("Model with Data Augmentation Test Loss:", model_DA_score[0])
print("Model with Data Augmentation Test Accuracy:", model_DA_score[1])

"""# 7.0 Fine tuning
fist warm up trails
"""

mobile_model = MobileNetV2(input_shape=(150, 150,3), include_top=False, weights='imagenet')

mobile_model.trainable = True

print("Number of layers in the MobileNetV2 model: ", len(mobile_model.layers))

fine_tune_at = 100
for layer in mobile_model.layers[:fine_tune_at]:
    layer.trainable =  False

# Create model adding the pre-trained model mobileNetV2, 
# adding GlobalAveragePooling2D layer
model = Sequential([mobile_model,
                    GlobalAveragePooling2D(),
                    Dropout(rate=0.5),
                    Dense(6, activation='softmax')])

model.compile(optimizer= RMSprop(lr=2e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Fitting model and save weights
trained_FT = model.fit(train_generator,
                    epochs=20,
                    validation_data=test_generator,
                    callbacks=[early_stopping])

model.save_weights('weights_FT.h5')

plot_acc_loss(trained_FT)

model.load_weights('weights_FT.h5')
model_FT_score = model.evaluate(test_generator)
print("Model Fine Tuning Test Loss:", model_FT_score[0])
print("Model Fine Tuning Test Accuracy:", model_FT_score[1])

img1 = image.load_img('/content/seg_pred/seg_pred/5.jpg', target_size=(150, 150))
x = image.img_to_array(img1)
x = np.expand_dims(x, axis=0)
prediction1 = model.predict(x, batch_size=10)

img2 = image.load_img('/content/seg_pred/seg_pred/176.jpg', target_size=(150, 150))
y = image.img_to_array(img2)
y = np.expand_dims(y, axis=0)
prediction2 = model.predict(y, batch_size=10)

plt.figure()
plt.subplot(121)
plt.title("Predicted class: " + str(np.argmax(prediction1[0])))
plt.imshow(img1)
plt.subplot(122)
plt.title("Predicted class: " + str(np.argmax(prediction2[0])))
plt.imshow(img2)

"""# 8.0 Conclusions of baseline evaluation

First results, notes, next steps

## CNN Model --> 84.7 %

* With standard CNN tools, it seems that the accuracy does not improve as much but reduces overfitting.
* With a CNN we already get a quit good result, with 84% accuracy in the test set.
"""

print("Model CNN Test Loss:", model_CNN_score[0])
print("Model CNN Test Accuracy:", model_CNN_score[1])

"""## Augmentation (general) --> 86.9%"""

print("Model with Data Augmentation Test Loss:", model_DA_score[0])
print("Model with Data Augmentation Test Accuracy:", model_DA_score[1])

"""## Fine Tuning (general) --> 92% acc"""

print("Model Fine Tuning Test Loss:", model_FT_score[0])
print("Model Fine Tuning Test Accuracy:", model_FT_score[1])

"""## Further improvements steps:

by additonal fine-tuning and/or additional data preproxessing to be evaluated:

* ...
* ...

## Save Evaluation Data
"""

!zip results.zip *.*

!zip -r /content/seg_pred.zip /content/seg_pred/seg_pred
!zip -r /content/seg_test.zip /content/seg_test/seg_test
!zip -r /content/seg_train.zip /content/seg_train/seg_train

!ls

